{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pneumonia_classify.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py pyyaml #to download and save the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvDZQhkOMtxs",
        "outputId": "a2f1f33a-ae15-4eeb-f379-a3c14c263a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras Libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import os\n",
        "\n",
        "train_folder= '/content/drive/MyDrive/chest_xray/train'\n",
        "val_folder = '/content/drive/MyDrive/chest_xray/val/'\n",
        "test_folder = '/content/drive/MyDrive/chest_xray/test'\n",
        "\n",
        "checkpoint_path = \"/content/checkpoints/model.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                       save_weights_only = True,\n",
        "                                       verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "# train \n",
        "os.listdir(train_folder)\n",
        "train_n = train_folder+'NORMAL/'\n",
        "train_p = train_folder+'PNEUMONIA/'\n",
        "\n",
        "cnn = Sequential()\n",
        "\n",
        "#Convolution\n",
        "cnn.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3)))\n",
        "\n",
        "#Pooling\n",
        "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# 2nd Convolution\n",
        "cnn.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "\n",
        "# 2nd Pooling layer\n",
        "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Flatten the layer\n",
        "cnn.add(Flatten())\n",
        "\n",
        "# Fully Connected Layers\n",
        "cnn.add(Dense(activation = 'relu', units = 128))\n",
        "cnn.add(Dense(activation = 'sigmoid', units = 1))\n",
        "\n",
        "# Compile the Neural network\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(train_folder,\n",
        "                                           target_size = (64, 64),\n",
        "                                           batch_size = 32,\n",
        "                                           class_mode = 'binary')\n",
        "\n",
        "validation_set = test_datagen.flow_from_directory(val_folder,\n",
        "                                             target_size=(64, 64),\n",
        "                                             batch_size=32,\n",
        "                                             class_mode='binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(test_folder,\n",
        "                                       target_size = (64, 64),\n",
        "                                       batch_size = 32,\n",
        "                                       class_mode = 'binary')\n",
        "\n",
        "\n",
        "cnn_model = cnn.fit(training_set,\n",
        "                    steps_per_epoch = 160,\n",
        "                    epochs = 5,\n",
        "                    validation_data = validation_set,\n",
        "                    validation_steps = 624)\n",
        "\n",
        "test_accu = cnn.evaluate(test_set,steps=624)\n",
        "\n",
        "print('The testing accuracy is :',test_accu[1]*100, '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGPAddb39vaS",
        "outputId": "be2cc69b-eb19-4520-8b82-749636ff982f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5106 images belonging to 2 classes.\n",
            "Found 126 images belonging to 2 classes.\n",
            "Found 625 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.4212 - accuracy: 0.8145WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 624 batches). You may need to use the repeat() function when building your dataset.\n",
            "160/160 [==============================] - 328s 2s/step - loss: 0.4212 - accuracy: 0.8145 - val_loss: 0.2359 - val_accuracy: 0.9048\n",
            "Epoch 2/5\n",
            "160/160 [==============================] - 58s 365ms/step - loss: 0.2456 - accuracy: 0.8982\n",
            "Epoch 3/5\n",
            "160/160 [==============================] - 60s 374ms/step - loss: 0.2090 - accuracy: 0.9089\n",
            "Epoch 4/5\n",
            "160/160 [==============================] - 59s 370ms/step - loss: 0.2022 - accuracy: 0.9205\n",
            "Epoch 5/5\n",
            "160/160 [==============================] - 59s 368ms/step - loss: 0.1755 - accuracy: 0.9307\n",
            " 20/624 [..............................] - ETA: 1:15:24 - loss: 0.4790 - accuracy: 0.8192WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 624 batches). You may need to use the repeat() function when building your dataset.\n",
            "624/624 [==============================] - 143s 228ms/step - loss: 0.4790 - accuracy: 0.8192\n",
            "The testing accuracy is : 81.91999793052673 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"model.tf\", \"w\")\n",
        "cnn.save_weights(\"model.HDF5\", save_format=\"tf\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "FMdqONkKRjn8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}